概述
----

### 硬件加速器设计的意义

未来人工智能发展的三大核心要素是数据、算法和算力。目前，人工智能系统算力大都构建在CPU+GPU之上，主体多是GPU。随着神经网络的层数越多，模型体量越大，算法越复杂，CPU和GPU很难再满足新型网络对于算力的需求。例如，2015年谷歌的AlphaGo与樊麾对弈时，用了1202个CPU和176个GPU，每盘棋需要消耗上千美元的电费，而与之对应的是樊麾的功耗仅为20瓦。

虽然GPU在面向向量、矩阵以及张量的计算上，引入许多新颖的优化设计，但由于GPU需要支持的计算类型复杂，芯片规模大、能耗高，人们开始将更多的精力转移到深度学习硬件加速器的设计上来。和传统CPU和GPU芯片相比，新型深度学习加速器会有更高的性能，以及更低的能耗。未来随着人们真正进入智能时代，智能应用的普及会越来越广泛，到那时每台服务器、每台智能手机、每个智能摄像头，都需要使用加速器。

### 硬件加速器设计的思路
:label:`accelerator-design-title`

近些年来，计算机体系结构的研究热点之一就是深度学习硬件加速器的设计。在体系结构的研究中，能效和通用性是两个重要的衡量指标。能效关注单位能耗下基本计算的次数，通用性主要指芯片能够覆盖的任务种类。

以两类特殊的芯片为例:一种是我们较为熟悉的通用处理器(如CPU)，该类芯片理论上可以完成各种计算任务，但是其能效较低大约只有0.1TOPS/W；另一种是专用集成电路(Application Specific Integrated Circuit, ASIC)，其能效更高，但是支持的任务相对而言就比较单一。对于通用的处理器而言，为了提升能效，在芯片设计上有许多加速技术的引入，例如：超标量技术、单指令多数据（Single Instruction Multiple Data，SIMD）技术以及单指令多线程（Single Instruction Multiple Threads，SIMT）技术等。

对于不同的加速器设计方向，业界也有不同的硬件实现。针对架构的通用性，NVIDIA持续在其GPU芯片上发力，先后推出了Volta, Turing, Ampere架构，并推出用于加速矩阵计算的张量核（Tensor Core），以满足深度学习海量算力的需求。

对于偏定制化的硬件架构，面向深度学习计算任务，业界提出了特定领域架构(Domain Specific Architecture)。 Google公司推出了TPU芯片，专门用于加速深度学习计算任务，其使用脉动阵列(Systolic Array)来优化矩阵乘法和卷积运算，可以充分地利用数据局部性，降低对内存的访问次数。华为也推出了自研的昇腾AI处理器，旨在为用户提供更高能效的算力和易用的开发、部署体验，其中的CUBE运算单元，就用于加速矩阵乘法的计算。
