@misc{nvidia-nccl,
    title={Nvidia Collective Communications Library (NCCL)},
    url={https://developer.nvidia.com/nccl},
    author={NVIDIA},
    year={2022}, month={Feb}
} 

@inproceedings{10.1007/978-3-030-50743-5_3,
    author="Graham, Richard L.
    and Levi, Lion
    and Burredy, Devendar
    and Bloch, Gil
    and Shainer, Gilad
    and Cho, David
    and Elias, George
    and Klein, Daniel
    and Ladd, Joshua
    and Maor, Ophir
    and Marelli, Ami
    and Petrov, Valentin
    and Romlet, Evyatar
    and Qin, Yong
    and Zemah, Ido",
    editor="Sadayappan, Ponnuswamy
    and Chamberlain, Bradford L.
    and Juckeland, Guido
    and Ltaief, Hatem",
    title="Scalable Hierarchical Aggregation and Reduction Protocol (SHARP)TM Streaming-Aggregation Hardware Design and Evaluation",
    booktitle="High Performance Computing",
    year="2020",
    publisher="Springer International Publishing",
    address="Cham",
    pages="41--59",
    abstract="This paper describes the new hardware-based streaming-aggregation capability added to Mellanox's Scalable Hierarchical Aggregation and Reduction Protocol in its HDR InfiniBand switches. For large messages, this capability is designed to achieve reduction bandwidths similar to those of point-to-point messages of the same size, and complements the latency-optimized low-latency aggregation reduction capabilities, aimed at small data reductions. MPI{\_}Allreduce() bandwidth measured on an HDR InfiniBand based system achieves about 95{\%} of network bandwidth. For medium and large data reduction this also improves the reduction bandwidth by a factor of 2--5 relative to host-based (e.g., software-based) reduction algorithms. Using this capability also increased DL-Poly and PyTorch application performance by as much as 4{\%} and 18{\%}, respectively. This paper describes SHARP Streaming-Aggregation hardware architecture and a set of synthetic and application benchmarks used to study this new reduction capability, and the range of data sizes for which Streaming-Aggregation performs better than the low-latency aggregation algorithm.",
    isbn="978-3-030-50743-5"
}

@misc{https://doi.org/10.48550/arxiv.2005.14165,
  doi = {10.48550/ARXIV.2005.14165},
  url = {https://arxiv.org/abs/2005.14165},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language Models are Few-Shot Learners},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@BOOK{Sanders2019-cq,
  title     = "Sequential and parallel algorithms and data structures",
  author    = "Sanders, Peter and Mehlhorn, Kurt and Dietzfelbinger, Martin and
               Dementiev, Roman",
  publisher = "Springer Nature",
  edition   =  1,
  month     =  sep,
  year      =  2019,
  address   = "Cham, Switzerland"
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@misc{li2022ddp, 
  title={Getting started with distributed data parallel},
  url={https://pytorch.org/tutorials/intermediate/ddp_tutorial.html},
  year={2022},
  journal={Getting Started with Distributed Data Parallel - PyTorch Tutorials 1.11.0+cu102 documentation},
  author={Li, Shen}, editor={Zhu, Joe}
}